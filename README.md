# ai

## LLMs

### Open Sourced

Here is a list of the top open-source large language models (LLMs) as of February 2025, based on their performance, versatility, and community adoption:

---

#### 1. **Llama 3.1 (Meta AI)**  
- **Parameters**: 8B to 405B  
- **Key Features**: Multilingual support, 128K token context window, strong reasoning and coding capabilities, and multimodal processing (text, audio, images, video) .  
- **Use Cases**: General-purpose text generation, multilingual applications, code generation, and long-form content creation.  

---

#### 2. **DeepSeek-V3 (DeepSeek AI)**  
- **Parameters**: 671B total (37B active)  
- **Key Features**: Advanced reasoning and coding capabilities, multilingual support, and fine-tuning for specific tasks. It is optimized for efficiency and performance .  
- **Use Cases**: General text generation, multilingual tasks, code generation, and advanced reasoning.

Here is the release timeline and key information for major versions of DeepSeek:

1. **DeepSeek-V1**  
   - **Release Date**: January 2024  
   - **Features**: The initial version focused on natural language processing and coding tasks, supporting multiple programming languages with a context window of up to 128K tokens.

2. **DeepSeek-V2 Series**  
   - **Release Date**: First half of 2024  
   - **Features**: Significant performance improvements, open-source and free for commercial use, with low training costs but slower inference speeds.

3. **DeepSeek-V2.5 Series**  
   - **Release Date**: September 2024  
   - **Features**: Combined Chat and Coder models, enhanced mathematical reasoning and writing capabilities, and added internet search functionality.

4. **DeepSeek-V3**  
   - **Release Date**: December 26, 2024  
   - **Features**: Utilized a Mixture of Experts (MoE) architecture with 671 billion parameters, rivaling top models like GPT-4o, and supported open-source and local deployment.

5. **DeepSeek-R1 Series**  
   - **Release Date**: January 20, 2025  
   - **Features**: Focused on reasoning capabilities, improved performance in mathematical and coding tasks through reinforcement learning, and supported model distillation.

For more detailed information, you can refer to relevant sources or visit DeepSeek's official website.

---

#### 3. **Falcon 180B (Technology Innovation Institute)**  
- **Parameters**: 180B  
- **Key Features**: Trained on 3.5 trillion tokens, excels in reasoning and coding tasks, and supports multilingual applications .  
- **Use Cases**: General text generation, code generation, mathematical tasks, and scientific knowledge applications .  

---

#### 4. **BLOOM (BigScience)**  
- **Parameters**: 176B  
- **Key Features**: Multilingual support for 46 natural languages and 13 programming languages, with a focus on open access and transparency .  
- **Use Cases**: Text summarization, translation, document classification, and creative content generation .  

---

#### 5. **Mixtral 8x22B (Mistral AI)**  
- **Parameters**: 141B total (39B active)  
- **Key Features**: Multilingual fluency (English, French, Italian, Spanish), high performance in math and programming tasks, and efficient resource usage .  
- **Use Cases**: Programming tasks, multilingual text generation, and complex reasoning .  

---

#### 6. **Vicuna 13-B (LMSYS)**  
- **Parameters**: 13B  
- **Key Features**: Fine-tuned on user-shared conversations, excels in conversational AI, and provides human-like responses .  
- **Use Cases**: Chatbots, customer support, and conversational AI applications .  

---

#### 7. **GPT-NeoX-20B (EleutherAI)**  
- **Parameters**: 20B  
- **Key Features**: Strong few-shot reasoning capabilities, open-source weights, and efficient inference .  
- **Use Cases**: Content generation, question answering, and code understanding .  

---

#### 8. **StableLM 2 (Stability AI)**  
- **Parameters**: 1.6B to 12B  
- **Key Features**: Multilingual text generation, code understanding, and fine-tuning for specific tasks .  
- **Use Cases**: Research, commercial applications, and multilingual content generation .  

---

#### 9. **Gemma 2 (Google)**  
- **Parameters**: 2B to 27B  
- **Key Features**: Optimized for efficient inference, responsible AI development, and strong performance for its size .  
- **Use Cases**: General text generation, question answering, and summarization .  

---

#### 10. **Mistral-7B (Mistral AI)**  
- **Parameters**: 7B  
- **Key Features**: Compact yet powerful, energy-efficient, and strong ethical guidelines .  
- **Use Cases**: Creative writing, coding assistance, and content generation .  

## Develoepr's Tool

### Cloud Service

| Source | Started Price |
| --- | --- | 
| https://lightning.ai/, Model Training | $3.39/hr |
| https://www.alibabacloud.com/en/product/ecs-pricing-list | $2.38/hr |
| https://aws.amazon.com/ec2/instance-types/g5/ |  $1.006/hr |
| https://azure.microsoft.com/en-us/pricing/details/machine-learning/ | $0.9/hr |
| https://hpc-ai.com/ H100 | $1.99/hr |
| https://www.digitalocean.com/pricing/gen-ai (Llama) | $0.68/mtokens |

### AI Training Tools

* https://cloud.google.com/use-cases/free-ai-tools
* https://aws.amazon.com/free/ai/

## Courses

* https://fortune.com/education/articles/free-ai-classes-you-can-take-online/
* https://cloud.google.com/learn/training/machinelearning-ai
* https://www.reddit.com/r/learnmachinelearning/comments/1czsras/what_are_the_best_free_online_ml_courses/

## Reference

* https://www.orbussoftware.com/product/why-orbus/awards-and-recognition/forrester-wave-report-download
* https://neueda.com/enterprise-learning/resources/blogs/ai-in-software-architecture/
* https://site.co-architecture.com/artificial-intelligence-ai/top-14-ai-tools-for-architects-and-designers/
* https://www.infoq.com/articles/architectural-intelligence/
* https://www.oreilly.com/radar/software-architecture-in-an-ai-world/

## Industry Samples

* https://openai.com/, GenAI
* https://www.deepseek.com/, GenAI
* https://www.anthropic.com/, GenAI
* https://www.meta.ai/, GenAI
* https://x.ai/, GenAI
* https://slate.ai/, Construction AI

## Hire me

Weijing Lin is a highly experienced up to staff level (L6). Within 10 years working experience, 
he has led multiple projects across multiple functional teams and success delivered the products 
to the market and help company grow including design, build the product from the scratch to 
generated tens billions revenues.

https://www.linkedin.com/in/weijingjaylin/
